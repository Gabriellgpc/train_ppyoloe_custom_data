
################################
#  Default Hyperparameters
################################

defaults:
  - default_train_params
  - ppyoloe_arch_params
  - _self_

################################
#  Architecture Hyperparameters
################################

depth_mult: 0.33
width_mult: 0.50

backbone:
  pretrained_weights: https://deci-pretrained-models.s3.amazonaws.com/ppyolo_e/CSPResNetb_s_pretrained.pth

################################
#  Training hyperparameters
################################

max_epochs: 10 #500
static_assigner_end_epoch: 5 #150

warmup_mode: "linear_batch_step"
warmup_initial_lr:  1e-6
lr_warmup_steps: 1000
lr_warmup_epochs: 0

initial_lr:  2e-3
lr_mode: cosine
cosine_final_lr_ratio: 0.1

zero_weight_decay_on_bias_and_bn: False
batch_accumulate: 1

# save_ckpt_epoch_list: [200, 250, 300, 350, 400, 450]

loss:
  ppyoloe_loss:
    num_classes: ${arch_params.num_classes}
    reg_max: ${arch_params.head.reg_max}

optimizer: AdamW
optimizer_params:
  weight_decay: 0.0001

ema: True
ema_params:
  decay: 0.9997
  decay_type: threshold

mixed_precision: False
sync_bn: True

valid_metrics_list:
  - DetectionMetrics:
      score_thres: 0.1
      top_k_predictions: 300
      num_cls: ${arch_params.num_classes}
      normalize_targets: True
      post_prediction_callback:
        _target_: super_gradients.training.models.detection_models.pp_yolo_e.PPYoloEPostPredictionCallback
        score_threshold: 0.01
        nms_top_k: 1000
        max_predictions: 300
        nms_threshold: 0.7

pre_prediction_callback:

phase_callbacks:
  - PPYoloETrainingStageSwitchCallback:
      static_assigner_end_epoch: ${training_hyperparams.static_assigner_end_epoch}

metric_to_watch: 'mAP@0.50:0.95'
greater_metric_to_watch_is_better: True

################################
#     Dataset/Dataloader
################################

train_dataset_params:
  data_dir: /datasets/hard_hat # root path to data
  subdir: train # sub directory path of data_dir containing the train data.
  # json_file: instances_train2017.json # path to coco train json file, data_dir/annotations/train_json_file.
  input_dim: [640, 640] # None, do not resize dataset on load
  cache_dir:
  cache: False
  transforms:
    - DetectionRandomAffine:
        degrees: 0                    # rotation degrees, randomly sampled from [-degrees, degrees]
        translate: 0.25               # image translation fraction
        scales: [ 0.5, 1.5 ]          # random rescale range (keeps size by padding/cropping) after mosaic transform.
        shear: 0.0                    # shear degrees, randomly sampled from [-degrees, degrees]
        target_size:
        filter_box_candidates: True   # whether to filter out transformed bboxes by edge size, area ratio, and aspect ratio.
        wh_thr: 2                     # edge size threshold when filter_box_candidates = True (pixels)
        area_thr: 0.1                 # threshold for area ratio between original image and the transformed one, when when filter_box_candidates = True
        ar_thr: 20                    # aspect ratio threshold when filter_box_candidates = True
    - DetectionRandomRotate90:
        prob: 0.5
    - DetectionRGB2BGR:
        prob: 0.25
    - DetectionHSV:
        prob: 0.5                       # probability to apply HSV transform
        hgain: 18                       # HSV transform hue gain (randomly sampled from [-hgain, hgain])
        sgain: 30                       # HSV transform saturation gain (randomly sampled from [-sgain, sgain])
        vgain: 30                       # HSV transform value gain (randomly sampled from [-vgain, vgain])
    - DetectionHorizontalFlip:
        prob: 0.5                       # probability to apply horizontal flip
    - DetectionMixup:
        input_dim:
        mixup_scale: [ 0.5, 1.5 ]         # random rescale range for the additional sample in mixup
        prob: 0.5                       # probability to apply per-sample mixup
        flip_prob: 0.5                  # probability to apply horizontal flip
    - DetectionNormalize:
        mean: [ 123.675, 116.28, 103.53 ]
        std: [ 58.395,  57.12,  57.375 ]
    - DetectionTargetsFormatTransform:
        output_format: LABEL_CXCYWH

  tight_box_rotation: False
  class_inclusion_list:
  max_num_samples:
  with_crowd: False

train_dataloader_params:
  batch_size: 8
  num_workers: 8
  shuffle: True
  drop_last: True
  # Disable pin_memory due to presence of PPYoloECollateFN with uses random resize during training
  pin_memory: False
  worker_init_fn:
    _target_: super_gradients.training.utils.utils.load_func
    dotpath: super_gradients.training.datasets.datasets_utils.worker_init_reset_seed
  collate_fn: # collate function for trainset
    _target_: super_gradients.training.utils.detection_utils.PPYoloECollateFN
    random_resize_sizes: [ 320, 352, 384, 416, 448, 480, 512, 544, 576, 608, 640, 672, 704, 736, 768 ]
    random_resize_modes:
      - 0 # cv::INTER_NEAREST
      - 1 # cv::INTER_LINEAR
      - 2 # cv::INTER_CUBIC
      - 3 # cv::INTER_AREA
      - 4 # cv::INTER_LANCZOS4

val_dataset_params:
  data_dir: /datasets/hard_hat # root path to coco data
  subdir: valid # sub directory path of data_dir containing the train data.
  # json_file: instances_val2017.json # path to coco train json file, data_dir/annotations/train_json_file.
  input_dim:
  cache_dir:
  cache: False
  transforms:
    - DetectionRescale:
        output_shape: [640, 640]
    - DetectionNormalize:
        mean: [ 123.675, 116.28, 103.53 ]
        std: [ 58.395,  57.12,  57.375 ]
    - DetectionTargetsFormatTransform:
        output_format: LABEL_CXCYWH
  tight_box_rotation: False
  class_inclusion_list:
  max_num_samples:
  with_crowd: True

val_dataloader_params:
  batch_size: 8
  num_workers: 8
  drop_last: False
  shuffle: False
  pin_memory: False
  collate_fn: # collate function for valset
    _target_: super_gradients.training.utils.detection_utils.CrowdDetectionPPYoloECollateFN

################################
#     Quantization Awareness
################################

ptq_only: False              # whether to launch QAT, or leave PTQ only
selective_quantizer_params:
  calibrator_w: "max"        # calibrator type for weights, acceptable types are ["max", "histogram"]
  calibrator_i: "histogram"  # calibrator type for inputs acceptable types are ["max", "histogram"]
  per_channel: True          # per-channel quantization of weights, activations stay per-tensor by default
  learn_amax: False          # enable learnable amax in all TensorQuantizers using straight-through estimator
  skip_modules:              # optional list of module names (strings) to skip from quantization

calib_params:
  histogram_calib_method: "percentile"  # calibration method for all "histogram" calibrators, acceptable types are ["percentile", "entropy", mse"], "max" calibrators always use "max"
  percentile: 99.99                     # percentile for all histogram calibrators with method "percentile", other calibrators are not affected
  num_calib_batches:                    # number of batches to use for calibration, if None, 512 / batch_size will be used
  verbose: False                        # if calibrator should be verbose

_convert_: all